{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7917ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruthienugula/RAG_learnings/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f36c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbd129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.\\nThe framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain enables Retrieval-Augmented Generation (RAG) by allowing developers to fetch relevant context before generating responses.\\nMemory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\\nBM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"FAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.\\nChroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.\\nThe 'stuff' chain sends all context at once to the LLM, useful for short documents in RAG.\"),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"The 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\\nThe 'refine' chain iteratively updates an answer by incorporating each new chunk of information.\\nLangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.\"),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\\nRAG pipelines in LangChain involve document loading, splitting, embedding, retrieval, and LLM-based response generation.\\nMMR (Maximal Marginal Relevance) retrieval in LangChain improves diversity by balancing relevance and redundancy.\\nTool usage in LangChain allows agents to execute predefined Python functions with contextual input from the user.'),\n",
       " Document(metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain supports reranking retrieved results using LLMs or neural cross-encoders to improve context quality.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Loading  and chunking  the document\n",
    "loader = TextLoader(\"langchain-rag-dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c4323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x126117a70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FAISS Vector Store with HuggingFace Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6abbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1410b7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x126f183b0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x12608af30>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=init_chat_model(\"groq:llama-3.3-70b-versatile\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edceda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Provide the answer to the question : {input} from the following context : {context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc28163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d97ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\":retriever,\n",
    "        \"input\": RunnablePassthrough()\n",
    "    } \n",
    "    | RunnablePassthrough.assign(\n",
    "        answer = prompt | llm | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08de22e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain supports agents and memory in the following ways:\n",
      "\n",
      "1. Agents: LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task. Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\n",
      "\n",
      "2. Memory: LangChain supports conversational memory using two types:\n",
      "   - ConversationBufferMemory: This allows models to retain previous interactions, making multi-turn conversations more coherent.\n",
      "   - ConversationSummaryMemory: This provides summarization memory, enabling models to summarize previous conversations and maintain context.\n",
      "\n",
      "By providing these features, LangChain enables the development of more complex and interactive LLM-based systems that can engage in multi-turn conversations and make use of various tools and memory mechanisms.\n",
      "[Document(id='68c7f7ab-1dac-4417-8ac8-33b65f5402fb', metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"The 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\\nThe 'refine' chain iteratively updates an answer by incorporating each new chunk of information.\\nLangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.\"), Document(id='a9f28483-0024-4d63-b02f-048a132ba7a8', metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.\\nThe framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.'), Document(id='57708514-1195-453a-a1e5-40438999c5d0', metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain enables Retrieval-Augmented Generation (RAG) by allowing developers to fetch relevant context before generating responses.\\nMemory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\\nBM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.'), Document(id='499e93a4-dd3b-4549-b77e-0c6f248543d7', metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"FAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.\\nChroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.\\nThe 'stuff' chain sends all context at once to the LLM, useful for short documents in RAG.\"), Document(id='a70fc29b-e97e-449c-aa16-14f41d19cb21', metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain supports reranking retrieved results using LLMs or neural cross-encoders to improve context quality.')]\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"How does Langchain support agents and memory ?\")\n",
    "print(response[\"answer\"])\n",
    "print(response[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2e2cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(id='68c7f7ab-1dac-4417-8ac8-33b65f5402fb', metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"The 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\\nThe 'refine' chain iteratively updates an answer by incorporating each new chunk of information.\\nLangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.\"),\n",
       "  Document(id='a9f28483-0024-4d63-b02f-048a132ba7a8', metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.\\nThe framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.'),\n",
       "  Document(id='57708514-1195-453a-a1e5-40438999c5d0', metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain enables Retrieval-Augmented Generation (RAG) by allowing developers to fetch relevant context before generating responses.\\nMemory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\\nBM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.'),\n",
       "  Document(id='499e93a4-dd3b-4549-b77e-0c6f248543d7', metadata={'source': 'langchain-rag-dataset.txt'}, page_content=\"FAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.\\nChroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.\\nThe 'stuff' chain sends all context at once to the LLM, useful for short documents in RAG.\"),\n",
       "  Document(id='a70fc29b-e97e-449c-aa16-14f41d19cb21', metadata={'source': 'langchain-rag-dataset.txt'}, page_content='LangChain supports reranking retrieved results using LLMs or neural cross-encoders to improve context quality.')],\n",
       " 'input': 'How does Langchain support agents and memory ?',\n",
       " 'answer': 'LangChain supports agents and memory in the following ways:\\n\\n1. Agents: LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task. Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\\n\\n2. Memory: LangChain supports conversational memory using two types:\\n   - ConversationBufferMemory: This allows models to retain previous interactions, making multi-turn conversations more coherent.\\n   - ConversationSummaryMemory: This provides summarization memory, enabling models to summarize previous conversations and maintain context.\\n\\nBy providing these features, LangChain enables the development of more complex and interactive LLM-based systems that can engage in multi-turn conversations and make use of various tools and memory mechanisms.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
