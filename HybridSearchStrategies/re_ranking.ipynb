{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c1ca75",
   "metadata": {},
   "source": [
    "Re Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f64be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maruthienugula/RAG_learnings/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facf9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "loader = TextLoader(\"langchain-sample.txt\")\n",
    "raw_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2b64e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split raw docs into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0bd9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can i use langchain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dda44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e66acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.from_documents(docs,embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3685ed10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x128550a40>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe47a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x127fbe3c0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x139641d30>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt and use the llm\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(\"groq:llama-3.3-70b-versatile\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b227e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- When replying give the output first then all relevant content\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65140b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d4860aab-e49c-40ee-a2b5-ca02b9724262', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='7f56a373-e3f6-4ab0-8494-224183a7b784', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='7a52ea98-9a38-492f-99f1-0c2be1e7ad32', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='1654f286-c1ac-4fd8-b6b1-f8f1d88fc9b5', metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='e085bb8d-aae5-4b55-8574-a18a18696b70', metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='23844a3e-82aa-4e29-b67f-7091aaa5fcca', metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb6c590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\n\\nUser Question: \"{question}\"\\n\\nDocuments:\\n{documents}\\n\\nInstructions:\\n- When replying give the output first then all relevant content\\n- Think about the relevance of each document to the user\\'s question.\\n- Return a list of document indices in ranked order, starting from the most relevant.\\n\\nOutput format: comma-separated document indices (e.g., 2,1,3,0,...)\\n')\n",
       "| ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x127fbe3c0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x139641d30>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "692a7d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1.LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents. with metadata : {'source': 'langchain-sample.txt'}\",\n",
       " \"2.LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation. with metadata : {'source': 'langchain-sample.txt'}\",\n",
       " \"3.LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful. with metadata : {'source': 'langchain-sample.txt'}\",\n",
       " \"4.FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution. with metadata : {'source': 'langchain-sample.txt'}\",\n",
       " \"5.Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search. with metadata : {'source': 'langchain-sample.txt'}\",\n",
       " \"6.Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity. with metadata : {'source': 'langchain-sample.txt'}\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lines = [f\"{i+1}.{doc.page_content} with metadata : {doc.metadata}\" for i,doc in enumerate(retrieved_docs)]\n",
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f2a6795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1.LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents. with metadata : {'source': 'langchain-sample.txt'}\\n2.LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation. with metadata : {'source': 'langchain-sample.txt'}\\n3.LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful. with metadata : {'source': 'langchain-sample.txt'}\\n4.FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution. with metadata : {'source': 'langchain-sample.txt'}\\n5.Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search. with metadata : {'source': 'langchain-sample.txt'}\\n6.Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity. with metadata : {'source': 'langchain-sample.txt'}\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_docs = \"\\n\".join(doc_lines)\n",
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "167af6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,1,4,2,6,5\\n\\nThe ranking is based on the relevance of each document to the user\\'s question about using LangChain to build an application with memory and tools. \\n\\n1. Document 3 is the most relevant because it explicitly mentions \"Memory in LangChain\" and its role in enabling context retention across multiple steps in a conversation or task, as well as tool integration.\\n2. Document 1 is the second most relevant as it provides a general overview of LangChain, including its components for prompt management, chains, memory, and agents, which are all relevant to building an application.\\n3. Document 4 is also relevant because it mentions \"Memory\" and \"Agents\" in LangChain, which can be used to decide which tools to use and in what order, making it suitable for multi-step tasks.\\n4. Document 2 is somewhat relevant as it talks about LangChain\\'s integration with third-party services, but it doesn\\'t directly address memory and tools.\\n5. Document 6 is less relevant as it discusses Retrieval-Augmented Generation (RAG) and vector databases, which, although related to LangChain, are not directly related to building an application with memory and tools.\\n6. Document 5 is the least relevant as it focuses on dense retrieval and hybrid retrieval, which, although related to LangChain, are not directly related to building an application with memory and tools.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fba73e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 3, 1, 5]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [int(x.strip())-1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50ed91b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d4860aab-e49c-40ee-a2b5-ca02b9724262', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='7f56a373-e3f6-4ab0-8494-224183a7b784', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='7a52ea98-9a38-492f-99f1-0c2be1e7ad32', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='1654f286-c1ac-4fd8-b6b1-f8f1d88fc9b5', metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='e085bb8d-aae5-4b55-8574-a18a18696b70', metadata={'source': 'langchain-sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='23844a3e-82aa-4e29-b67f-7091aaa5fcca', metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac8b1e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7a52ea98-9a38-492f-99f1-0c2be1e7ad32', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='d4860aab-e49c-40ee-a2b5-ca02b9724262', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='1654f286-c1ac-4fd8-b6b1-f8f1d88fc9b5', metadata={'source': 'langchain-sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='7f56a373-e3f6-4ab0-8494-224183a7b784', metadata={'source': 'langchain-sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='23844a3e-82aa-4e29-b67f-7091aaa5fcca', metadata={'source': 'langchain-sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0<=i<=len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f17f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final re-ranked results\n",
      "\n",
      "Rank 1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 3. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "\n",
      "Rank 4. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "\n",
      "Rank 5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(\"Final re-ranked results\")\n",
    "for i,doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nRank {i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74492727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
