{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69e27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\" #so that langsmith can trace through the dataset and things happening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07a051",
   "metadata": {},
   "source": [
    "Creating a reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8697a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['54800205-e738-4505-b582-6dddb7670dd4',\n",
       "  '8fcc5170-4ca5-4f4c-b97b-ccd886fe82ac',\n",
       "  '09787139-a515-4c0f-9b06-e93cdd157cd2',\n",
       "  '5018fdfc-50a2-41b0-ae87-4eec7871a0ee',\n",
       "  '51f62c1d-4b6e-4fd9-96b3-550f5e11aee6'],\n",
       " 'count': 5,\n",
       " 'as_of': '2026-02-09T14:18:46.832669002Z'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"Chatbot Evaluation\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=[\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangChain?\"},\n",
    "            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangSmith?\"},\n",
    "            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is OpenAI?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Google?\"},\n",
    "            \"outputs\": {\"answer\": \"A technology company known for search\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Mistral?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11550f49",
   "metadata": {},
   "source": [
    "Defining metrics ( LLM as JUDGE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7314efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "from langsmith import wrappers\n",
    " \n",
    "llm_client = wrappers.wrap_openai(\n",
    "    OpenAI(\n",
    "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "        base_url=\"https://api.groq.com/openai/v1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "eval_instructions = \"You are an expert professor specialized in grading students' answers to questions.Be 100 percent sure with the decisions\"\n",
    "\n",
    "def correctness(inputs:dict,outputs:dict, reference_outputs:dict)->bool:\n",
    "      user_content = f\"\"\"You are grading the following question:\n",
    "    {inputs['question']}\n",
    "    Here is the real answer:\n",
    "    {reference_outputs['answer']}\n",
    "    You are grading the following predicted answer:\n",
    "    {outputs.get('response', '')}\n",
    "\n",
    "    Just Respond with CORRECT or INCORRECT and don't reply with any other details:\n",
    "    Grade:\n",
    "    \"\"\"\n",
    "      response=llm_client.chat.completions.create(\n",
    "            model=\"openai/gpt-oss-20b\",\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                  {\"role\":\"system\",\"content\":eval_instructions},\n",
    "                  {\"role\":\"user\",\"content\":user_content}\n",
    "            ]\n",
    "      ).choices[0].message.content\n",
    "\n",
    "      return response.strip().upper().startswith(\"CORRECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e2d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concisions- checks whether the actual output is less than 2x the length of the expected result.\n",
    "\n",
    "def concision(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return int(len(outputs.get(\"response\", \"\")) < 2 * len(reference_outputs[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2122a20",
   "metadata": {},
   "source": [
    "Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dabfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "def my_app(question: str, model: str = \"openai/gpt-oss-20b\", instructions: str = default_instructions) -> str:\n",
    "    return llm_client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123ad47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call my_app for every datapoints\n",
    "def ls_target(inputs: str) -> dict:\n",
    "    return {\"response\": my_app(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c59fe353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'openai/gpt-oss-120b-18aa54a5' at:\n",
      "https://smith.langchain.com/o/a6f4c2f1-2bbf-45fd-af95-0534b24925de/datasets/e6874c3e-b755-4886-a84a-1c9d3a15c532/compare?selectedSessions=a3977d9a-26e5-4cea-b5d8-b0fcc592b82d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:12,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = client.evaluate(\n",
    "    ls_target,          # your system under test\n",
    "    data=dataset_name,  # LangSmith dataset\n",
    "    evaluators=[correctness, concision],\n",
    "    experiment_prefix=\"openai/gpt-oss-120b\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f8846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_learnings (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
